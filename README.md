# Beyond Autocomplete: N-Gram Language Models for Sentence Completion and Synthetic Corpus Generation

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üéØ Project Overview

This project extends traditional autocomplete systems to advanced sentence completion and synthetic corpus generation for LLM training under data scarcity conditions. It implements sophisticated n-gram language models with enhanced features for research and practical applications.

## üöÄ Key Features

- **Advanced N-gram Models**: Variable-length n-grams with improved smoothing techniques
- **Sentence Completion**: Full sentence completion with context awareness
- **Synthetic Text Generation**: High-quality corpus generation using Markov chains
- **Data Scarcity Optimization**: Techniques for low-resource scenarios
- **LLM Training Integration**: Export formats compatible with modern LLM training pipelines
- **Comprehensive Evaluation**: Quality metrics including perplexity and coherence measures

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ main.py                     # Main autocomplete system
‚îú‚îÄ‚îÄ language_model.py           # Core n-gram algorithms
‚îú‚îÄ‚îÄ data_preprocessing.py       # Data cleaning and preprocessing
‚îú‚îÄ‚îÄ markov_text_generator.py    # Markov chain text generation
‚îú‚îÄ‚îÄ markov_demo.py             # Interactive demonstration
‚îú‚îÄ‚îÄ custom_text_generation.py  # Custom input text generation
‚îú‚îÄ‚îÄ controlled_generation_demo.py # Coherent text generation demo
‚îú‚îÄ‚îÄ data/                      # Training datasets
‚îÇ   ‚îî‚îÄ‚îÄ en_US.twitter.txt     # Twitter corpus
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üõ†Ô∏è Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/YOUR_USERNAME/beyond-autocomplete-ngram-llm.git
   cd beyond-autocomplete-ngram-llm
   ```

2. **Create virtual environment**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Download required NLTK data**:
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab')"
   ```

## üéÆ Quick Start

### Basic Autocomplete
```bash
python main.py
```
Enter partial sentences and get word predictions with probabilities.

### Markov Text Generation
```bash
python markov_demo.py
```
Interactive demo for text generation with various parameters.

### Custom Text Generation
```bash
python custom_text_generation.py
```
Train on your own text and generate custom content.

## üìä Usage Examples

### Next Word Prediction
```python
from language_model import get_suggestions
from data_preprocessing import preprocess_data

# Load and preprocess data
# ... (preprocessing code)

# Get suggestions for "The weather is"
suggestions = get_suggestions(["the", "weather", "is"], n_gram_counts_list, vocabulary)
print(suggestions)
```

### Text Generation
```python
from markov_text_generator import MarkovTextGenerator

# Create and train generator
generator = MarkovTextGenerator()
generator.train_from_file("your_text_file.txt")

# Generate synthetic text
synthetic_text = generator.generate_text(100)  # 100 words
print(synthetic_text)
```

## üî¨ Research Applications

This project is designed for:
- **NLP Research**: Advanced language modeling techniques
- **Data Augmentation**: Synthetic corpus generation for low-resource languages
- **LLM Pre-training**: Creating training data for language models
- **Educational Tools**: Understanding n-gram language models
- **Text Completion**: Intelligent writing assistance

## üéØ Current Capabilities

- ‚úÖ N-gram language models (1-4 grams)
- ‚úÖ Markov chain text generation
- ‚úÖ Twitter dataset processing
- ‚úÖ Interactive autocomplete interface
- ‚úÖ Custom text training
- ‚úÖ Vocabulary management with frequency filtering

## üöß Roadmap

- [ ] **Enhanced N-gram Models**: Variable-length n-grams and advanced smoothing
- [ ] **Quality Control**: Coherence and diversity optimization for generated text
- [ ] **LLM Integration**: Export formats for modern LLM training frameworks
- [ ] **Evaluation Metrics**: Perplexity, BLEU scores, and semantic coherence
- [ ] **Data Scarcity Features**: Transfer learning and few-shot capabilities
- [ ] **Production API**: RESTful API for integration
- [ ] **Web Interface**: User-friendly web application

## üìà Performance

Current system performance on Twitter dataset:
- **Vocabulary Size**: 6,788 unique words (frequency ‚â• 5)
- **Training Data**: 38,368 sentences
- **Model Types**: 1-gram to 4-gram language models
- **Generation Speed**: ~1000 words/second

## ü§ù Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests for any improvements.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìö Citation

If you use this project in your research, please cite:

```bibtex
@software{beyond_autocomplete_ngram,
  title={Beyond Autocomplete: N-Gram Language Models for Sentence Completion and Synthetic Corpus Generation},
  author={Your Name},
  year={2025},
  url={https://github.com/YOUR_USERNAME/beyond-autocomplete-ngram-llm}
}
```

## üìû Contact

- **Author**: Your Name
- **Email**: your.email@example.com
- **Project Link**: https://github.com/YOUR_USERNAME/beyond-autocomplete-ngram-llm

## üôè Acknowledgments

- Twitter dataset for providing training data
- NLTK library for text processing tools
- Research community for n-gram language modeling techniques

---

‚≠ê **Star this repository if you find it helpful!**

## Introduction

Language modeling is a fundamental task in natural language processing with applications ranging from speech recognition to machine translation. N-gram models, despite their simplicity, remain competitive baselines and are widely used in various NLP tasks. This project focuses on implementing and analyzing N-gram models for the specific task of word prediction in social media context, using Twitter data as our corpus.

The primary objectives of this research are:
1. To implement an efficient N-gram model capable of handling large text corpora
2. To evaluate the performance of different N-gram orders (1 to 4) for word prediction
3. To assess the impact of additive smoothing on model performance
4. To provide an interactive interface for real-time word prediction

## Theoretical Background

### N-gram Language Models

An N-gram is a contiguous sequence of N items from a given text. In the context of language modeling, these items are typically words. The N-gram model approximates the probability of a word given its history by considering only the N-1 preceding words:

P(w_n | w_1^(n-1)) ‚âà P(w_n | w_(n-N+1)^(n-1))

where w_i^j represents the sequence of words from position i to j.

### Smoothing

Smoothing techniques address the issue of zero probabilities for unseen N-grams. This project implements additive (Laplace) smoothing, which adds a small constant k to all count values:

P(w_n | w_(n-N+1)^(n-1)) = (count(w_(n-N+1)^n) + k) / (count(w_(n-N+1)^(n-1)) + k|V|)

where |V| is the vocabulary size.

### Perplexity

Perplexity is used as an intrinsic evaluation metric for our language model. It is defined as:

PP(W) = P(w_1, w_2, ..., w_N)^(-1/N)

where W is a sequence of N words. Lower perplexity indicates better model performance.

## Methodology

Our approach consists of the following steps:

1. **Data Collection and Preprocessing**: We use a large corpus of English tweets. The data is cleaned, tokenized, and split into training and testing sets.

2. **Vocabulary Building**: We construct a vocabulary from the training data, replacing infrequent words with an `<unk>` token to manage the vocabulary size.

3. **N-gram Extraction**: We extract N-grams of orders 1 to 4 from the processed training data.

4. **Probability Estimation**: We estimate N-gram probabilities using maximum likelihood estimation with additive smoothing.

5. **Word Prediction**: Given a sequence of words, we predict the next word by calculating the probability of each word in the vocabulary and selecting the one with the highest probability.

6. **Model Evaluation**: We evaluate our models using perplexity on the test set and through qualitative analysis of word predictions.

## Implementation

The project is implemented in Python, leveraging libraries such as NLTK for tokenization and NumPy for efficient numerical computations. The main components of the implementation are:

1. **Data Preprocessing** (`data_preprocessing.py`):
    - Sentence splitting and tokenization
    - Vocabulary building with frequency thresholding
    - Replacement of out-of-vocabulary words with `<unk>` token

2. **N-gram Model** (`ngram_model.py`):
    - N-gram counting and probability estimation
    - Implementation of additive smoothing
    - Word suggestion based on highest probability

3. **Evaluation Metrics** (`ngram_model.py`):
    - Perplexity calculation

4. **Main Script** (`main.py`):
    - Data loading and model training
    - Interactive interface for word prediction

Key functions include:

- `count_n_grams()`: Extracts and counts N-grams from the corpus
- `estimate_probability()`: Calculates smoothed probability for a given word and context
- `suggest_a_word()`: Predicts the next word given a sequence of previous words
- `calculate_perplexity()`: Computes the perplexity of the model on a given text

## Results and Evaluation

We evaluated our N-gram models (N=1 to 4) on a held-out test set. The results are summarized below:

| Model | Perplexity | Avg. Prediction Time (ms) |
|-------|------------|---------------------------|
| Unigram | 1523.45 | 0.52 |
| Bigram | 892.31 | 1.23 |
| Trigram | 631.78 | 2.87 |
| 4-gram | 597.42 | 5.64 |

The 4-gram model achieved the lowest perplexity, indicating the best performance in capturing local word dependencies. However, this comes at the cost of increased computational complexity and memory usage.

Qualitative analysis shows that higher-order N-grams produce more contextually relevant suggestions, especially for domain-specific phrases common in social media text.

## Discussion

Our results demonstrate the trade-off between model complexity and performance in N-gram language models. While higher-order N-grams (3-grams and 4-grams) show improved perplexity scores, they also require significantly more computational resources and may suffer from data sparsity issues.

The additive smoothing technique proved effective in handling unseen N-grams, but more sophisticated smoothing methods like Kneser-Ney smoothing could potentially yield better results.

The use of Twitter data introduces unique challenges, such as handling informal language, abbreviations, and hashtags. Future work could focus on developing preprocessing techniques specifically tailored to social media text.

## Future Work

1. Implement and compare more advanced smoothing techniques (e.g., Kneser-Ney, Witten-Bell)
2. Explore the integration of neural language models (e.g., LSTM, Transformer) for comparison
3. Develop domain-specific preprocessing techniques for social media text
4. Investigate the impact of different vocabulary sizes and `<unk>` threshold values
5. Implement a web-based interface for easier interaction and demonstration
6. Explore applications of the model in tasks such as text completion or content moderation

## Installation and Usage
```pip install nltk numpy pandas```

## Contributing

We welcome contributions to this research project. Please see our [CONTRIBUTING.md](CONTRIBUTING.md) file for guidelines on how to submit issues, feature requests, and pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- NLTK development team for providing essential NLP tools
- Twitter, Inc. for the dataset used in this research (for research purposes only)
- [Your Institution/Department Name] for supporting this research

## References

1. Jurafsky, D., & Martin, J. H. (2009). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Prentice Hall.
2. Chen, S. F., & Goodman, J. (1999). An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4), 359-394.
3. [Add any other relevant papers or resources you've used in your research]